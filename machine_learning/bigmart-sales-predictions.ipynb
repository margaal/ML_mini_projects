{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Big Mart Sales Predictions","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"In this notebook, I am building a sales predictor based on BigMart Outlets data recolted by their data scientists. In the first time, I will try to understand every features of our data(Exploratory Data Analysis, EDA). Then, I will move to preprocessing step to prepare data to the final step, training model.\n\n#### Steps:\n===========\n- Information about data (with recall of the goal of work)\n- Exploratory Data Analysis (EDA)\n- Data preprocessing\n- Model training\n- Conclusion\n","metadata":{}},{"cell_type":"code","source":"# Setup notebook by importing necessary librairies\nimport numpy as np\nimport pandas as pd\npd.plotting.register_matplotlib_converters() # Register pandas formatters and converters with matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:46:02.894229Z","iopub.execute_input":"2021-10-06T17:46:02.894553Z","iopub.status.idle":"2021-10-06T17:46:02.906848Z","shell.execute_reply.started":"2021-10-06T17:46:02.894525Z","shell.execute_reply":"2021-10-06T17:46:02.905301Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"###### Information about dataset\nThis notebook has two main goals: make analysis of data and build Machine Learning model to predict the sales of each item at a particular outlet. We have 12 columns in our dataset.\n\n| Column          | Description       | Data Type    |\n|-----------------|-------------------|--------------|\n| Item_Identifier | Unique product ID | Alphanumeric |\n| Item_Weight     | Weight of product | Numeric      |\n| Item_Fat_Content | Wether the product is low fat or not | Alphanumeric |\n| Item_Visibility | The percent of total display area of all products in a store allocated to a particular product |  Numeric  |\n| Item_Type       |  Category of product |  Alphanumeric |\n| Item_MRP        |  Maximum Retail Price of product | Numeric             |\n| Outlet_Identifier| Unique store ID | Alphanumeric |\n| Item_Established_Year | The year in which store was created | Numeric |\n| Outlet_Size     | Size of the store in terms of ground area covered | Numeric |\n| Outlet_Location_Type | The type of city in which the store is located | Alphanumeric |\n| Outlet_Type | Whether the outlet is just a grocery store or some sort of supermarket | Alphanumeric |\n| Item_Outlet_Sales | Sales of the product in the particular store. This is the outcome variable to be predicted. | Numeric |","metadata":{}},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\nLet's to make some hypotheses about features of data. Let's ask what element can increase or decrease items sales for a supermarket?\n\n- The usefulness of product for the vast majority of customers. It is defined by product category or the brand of product, and so one... But here, based on our dataset, we can use \"Item_Type\" feature to estimate the kind of product is loved by customers.\n- The price of product can influence the sales. To valid this hypothese we will check correlation between Item_MRP and \"Item_Outlet_Sales\"\n- The visibility of product in the store: Column \"Item_Visibility\"\n- The place where the store is established: depending on the location of the store the product's price may be different so column \"Outlet_Location_Type\" is important\n\nThese hypotheses are subjective. but with further exploration of the data, we will accept or reject each of these assumptions","metadata":{}},{"cell_type":"markdown","source":"### Explore Data Analysis","metadata":{}},{"cell_type":"markdown","source":"Let's read dataset and make quick review","metadata":{}},{"cell_type":"code","source":"# Read data\nTRAIN_FILE_PATH = \"../input/bigmart-sales-data/Train.csv\"\ndata = pd.read_csv(TRAIN_FILE_PATH)\nprint(\"Shape of data: \", data.shape)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:45:09.251614Z","iopub.execute_input":"2021-10-06T16:45:09.252001Z","iopub.status.idle":"2021-10-06T16:45:09.304762Z","shell.execute_reply.started":"2021-10-06T16:45:09.251967Z","shell.execute_reply":"2021-10-06T16:45:09.303596Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:45:12.188950Z","iopub.execute_input":"2021-10-06T16:45:12.189315Z","iopub.status.idle":"2021-10-06T16:45:12.225207Z","shell.execute_reply.started":"2021-10-06T16:45:12.189283Z","shell.execute_reply":"2021-10-06T16:45:12.224321Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data.describe()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:45:12.998463Z","iopub.execute_input":"2021-10-06T16:45:12.998836Z","iopub.status.idle":"2021-10-06T16:45:13.032800Z","shell.execute_reply.started":"2021-10-06T16:45:12.998807Z","shell.execute_reply":"2021-10-06T16:45:13.032039Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"As we can see it, \"Item_Outlet_Sales\" is the target we must predict with our future model.","metadata":{}},{"cell_type":"code","source":"target = 'Item_Outlet_Sales'\nfeatures = [col for col in data.columns if col!=target]\nprint(\"We have\", len(features), \"features:\", features)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:45:16.945292Z","iopub.execute_input":"2021-10-06T16:45:16.945801Z","iopub.status.idle":"2021-10-06T16:45:16.952117Z","shell.execute_reply.started":"2021-10-06T16:45:16.945769Z","shell.execute_reply":"2021-10-06T16:45:16.951260Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# make two lists, one for categorial features names and second for others features\ncat_features = list(data[features].select_dtypes(include='object').columns)\nnum_features = list(set(features)-set(cat_features))\nprint(\"Categorical features:(\",len(cat_features),\")\", cat_features)\nprint(\"Numerical features(\",len(num_features),\"):\", num_features)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:45:56.794155Z","iopub.execute_input":"2021-10-06T16:45:56.794502Z","iopub.status.idle":"2021-10-06T16:45:56.805682Z","shell.execute_reply.started":"2021-10-06T16:45:56.794472Z","shell.execute_reply":"2021-10-06T16:45:56.804413Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# get name of columns which contains null values\ndef summarize_null_cols(X, cols):\n    null_columns = list(X[cols].isnull().sum()[X[cols].isnull().sum()>0].index)\n    print(\"Columns which contain null:\",null_columns)\n    for col in null_columns:\n        percent = (X[col].isnull().sum().sum()/len(X[col]))*100\n        print(\"For\", col,\"column we have\", round(percent,2),\"%of null values\")\nall_cols = features + [target]\nsummarize_null_cols(data, all_cols)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:41:07.306988Z","iopub.execute_input":"2021-10-06T17:41:07.307537Z","iopub.status.idle":"2021-10-06T17:41:07.335079Z","shell.execute_reply.started":"2021-10-06T17:41:07.307488Z","shell.execute_reply":"2021-10-06T17:41:07.334118Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"First observations we can make it is:\n- There are 4 numerical features and target\n- \"Item_Weight\" and \"Outlet_Size\" columns have some null entries, so we must do imputation in preprocessing step\n","metadata":{}},{"cell_type":"markdown","source":"### Numerical features exploratory","metadata":{}},{"cell_type":"markdown","source":"I make correlation figure analyse numericals features interaction with target","metadata":{}},{"cell_type":"code","source":"# plot correlation between numerical features and target\nfig = plt.figure()\ncorr = data[num_features+[target]].corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(240,10,as_cmap=True),\n            square=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:46:00.551512Z","iopub.execute_input":"2021-10-06T16:46:00.551865Z","iopub.status.idle":"2021-10-06T16:46:00.870272Z","shell.execute_reply.started":"2021-10-06T16:46:00.551836Z","shell.execute_reply":"2021-10-06T16:46:00.869304Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"plt.figure()\nplt.scatter(data['Item_MRP'], data[target])\nplt.xlabel(\"Item_MRP\")\nplt.ylabel(\"Item_Outlet_Sales\")","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:46:05.047329Z","iopub.execute_input":"2021-10-06T16:46:05.047698Z","iopub.status.idle":"2021-10-06T16:46:05.302950Z","shell.execute_reply.started":"2021-10-06T16:46:05.047667Z","shell.execute_reply":"2021-10-06T16:46:05.301828Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"If you remember, one of my assumptions above is this: Item sales can depend on the price of the items. With this graph, we can notice the strong correlation between \"Item_MRP\" and \"Item_Sales_Outlet\".\n<br>\nBut one thing seems odd to me about the correlation graph. I thought the correlation between \"Item_Visibility\" and \"Item_Outlet_Sales\" would have been higher than what I get because the more visible a product is the more likely it is to be sold, vice versa. Maybe it's due to the many values of 0.0 we have in the \"Item_Visibility\" columns? My answer is subjective (yes). Because having 0.0% visibility area for a product is inconceivable, I guess it's probably missing data and they set 0.0% by default.","metadata":{}},{"cell_type":"code","source":"plt.figure()\nsns.distplot(data[target], color=\"blue\")\nplt.show()\nprint(\"Skew (is positive): \", data[target].skew())","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:21:51.124310Z","iopub.execute_input":"2021-10-06T17:21:51.124885Z","iopub.status.idle":"2021-10-06T17:21:51.573694Z","shell.execute_reply.started":"2021-10-06T17:21:51.124824Z","shell.execute_reply":"2021-10-06T17:21:51.572850Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"Our target is skewed. We can make transformation like 'log' to reduce the skewness(We will do it later)","metadata":{}},{"cell_type":"markdown","source":"### Categorical features exploratory","metadata":{}},{"cell_type":"code","source":"for col in cat_features:\n    unique_val = data[col].unique()\n    print(col,'(', len(unique_val),'):',unique_val)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:46:11.517504Z","iopub.execute_input":"2021-10-06T16:46:11.517906Z","iopub.status.idle":"2021-10-06T16:46:11.538560Z","shell.execute_reply.started":"2021-10-06T16:46:11.517868Z","shell.execute_reply":"2021-10-06T16:46:11.537485Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Among the categorical characteristics, we can notice that there are ordinal characteristics. An ordinal characteristic is first of all a categorical characteristic which contains values which can be ordered. In our dataset we have:\n- 'Item_Fat_Content'\n- 'Outlet_Size'\n- 'Outlet_Type'\n- 'Outlet_Location_Type'\n\nFollowing anlysis is based on median value. When you have a skewed distribution, the median is a better measure of central tendency than the mean.","metadata":{}},{"cell_type":"code","source":"plt.figure()\ndata[['Outlet_Type', target]].groupby('Outlet_Type').median().plot(kind='bar')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:55:55.510002Z","iopub.execute_input":"2021-10-06T16:55:55.510435Z","iopub.status.idle":"2021-10-06T16:55:55.707701Z","shell.execute_reply.started":"2021-10-06T16:55:55.510395Z","shell.execute_reply":"2021-10-06T16:55:55.706652Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"plt.figure()\ndata[['Outlet_Location_Type', target]].groupby('Outlet_Location_Type').median().plot(kind='bar')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:56:40.223285Z","iopub.execute_input":"2021-10-06T16:56:40.223865Z","iopub.status.idle":"2021-10-06T16:56:40.388827Z","shell.execute_reply.started":"2021-10-06T16:56:40.223813Z","shell.execute_reply":"2021-10-06T16:56:40.387717Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"plt.figure()\ndata[['Item_Fat_Content', target]].groupby('Item_Fat_Content').median().plot.barh()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T16:58:37.736329Z","iopub.execute_input":"2021-10-06T16:58:37.736733Z","iopub.status.idle":"2021-10-06T16:58:37.912790Z","shell.execute_reply.started":"2021-10-06T16:58:37.736700Z","shell.execute_reply":"2021-10-06T16:58:37.911647Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Here,we can notice we need to preprocess this feature to make value uniform","metadata":{}},{"cell_type":"code","source":"plt.figure()\ndata[['Outlet_Size', target]].groupby('Outlet_Size').median().plot(kind='bar')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:00:25.831331Z","iopub.execute_input":"2021-10-06T17:00:25.831754Z","iopub.status.idle":"2021-10-06T17:00:25.988097Z","shell.execute_reply.started":"2021-10-06T17:00:25.831713Z","shell.execute_reply":"2021-10-06T17:00:25.987006Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data.pivot_table(values='Item_Outlet_Sales', index='Item_Type').sort_values(by='Item_Outlet_Sales').plot(kind=\"barh\")","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:01:53.466504Z","iopub.execute_input":"2021-10-06T17:01:53.466931Z","iopub.status.idle":"2021-10-06T17:01:53.768346Z","shell.execute_reply.started":"2021-10-06T17:01:53.466897Z","shell.execute_reply":"2021-10-06T17:01:53.767367Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"for otype in data['Outlet_Type'].unique():\n    print(\"\\n\",otype)\n    print(data[data['Outlet_Type']==otype]['Outlet_Size'].value_counts(dropna=False))","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:14:26.020380Z","iopub.execute_input":"2021-10-06T17:14:26.020841Z","iopub.status.idle":"2021-10-06T17:14:26.047659Z","shell.execute_reply.started":"2021-10-06T17:14:26.020804Z","shell.execute_reply":"2021-10-06T17:14:26.046754Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"We can notice that, supermarket Type1 is alone which have 'High' as Outlet Size. Supermarket Type2 and Type3 have only a Medium size for its outlets. This analysis will be helpful in preprocessing step to do imputation in order to give value which will have sense for Outlet_Size column","metadata":{}},{"cell_type":"code","source":"data[['Outlet_Identifier', target]].groupby('Outlet_Identifier').median().plot(kind='bar')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:19:43.181646Z","iopub.execute_input":"2021-10-06T17:19:43.182002Z","iopub.status.idle":"2021-10-06T17:19:43.369431Z","shell.execute_reply.started":"2021-10-06T17:19:43.181968Z","shell.execute_reply":"2021-10-06T17:19:43.368667Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing\n\nAs we saw it, our dataset contains null values in two columns: Item_Weight and Outlet_Size. By making exploratory we found some idea about how we can deal with NaN values in these columns. \n<br>\nFor 'Item_Weight' column, we have items which weights are known except 4 items (like you can see below). So for these 4 items we will use the mean value of 'Item_Weight'\n<br>","metadata":{}},{"cell_type":"code","source":"items_weight_mean = data[['Item_Identifier', 'Item_Weight']].groupby('Item_Identifier').mean()\nprint(items_weight_mean[items_weight_mean['Item_Weight'].isnull()])\nitems_weight_mean[items_weight_mean['Item_Weight'].isnull()] = data['Item_Weight'].mean()\nprint(items_weight_mean[items_weight_mean['Item_Weight'].isnull()])","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:39:20.466871Z","iopub.execute_input":"2021-10-06T17:39:20.467244Z","iopub.status.idle":"2021-10-06T17:39:20.482117Z","shell.execute_reply.started":"2021-10-06T17:39:20.467212Z","shell.execute_reply":"2021-10-06T17:39:20.481437Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def impute_item_weight(row):\n    item_id = row['Item_Identifier']\n    item_weight = row['Item_Weight']\n    \n    if not pd.isnull(item_weight):\n        return item_weight\n    # else\n    return items_weight_mean['Item_Weight'][items_weight_mean.index==item_id]\n    \n# impute item_weight\ndata['Item_Weight'] = data.apply(impute_item_weight, axis=1).astype(float)\nsummarize_null_cols(data, features)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:41:22.521590Z","iopub.execute_input":"2021-10-06T17:41:22.522161Z","iopub.status.idle":"2021-10-06T17:41:22.695906Z","shell.execute_reply.started":"2021-10-06T17:41:22.522113Z","shell.execute_reply":"2021-10-06T17:41:22.694928Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"markdown","source":"For 'Outlet_Size' column, we found previously a relation between Outlet_Size and Outlet_Type. We will use this to impute Outlet_Size by taking most frequent Size based on Outlet type.","metadata":{}},{"cell_type":"code","source":"most_outlet_size_by_type = data.pivot_table(values='Outlet_Size', columns='Outlet_Type', aggfunc=(lambda x: x.mode()[0]))\n\ndef impute_outlet_size(row):\n    outlet_type = row['Outlet_Type']\n    outlet_size = row['Outlet_Size']\n    \n    if not pd.isnull(outlet_size):\n        return outlet_size\n    return most_outlet_size_by_type.loc['Outlet_Size'][most_outlet_size_by_type.columns==outlet_type][0]\n\n# impute outlet_size\n\ndata['Outlet_Size'] = data.apply(impute_outlet_size, axis=1)\nsummarize_null_cols(data, features)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:41:57.617315Z","iopub.execute_input":"2021-10-06T17:41:57.618002Z","iopub.status.idle":"2021-10-06T17:41:58.329836Z","shell.execute_reply.started":"2021-10-06T17:41:57.617952Z","shell.execute_reply":"2021-10-06T17:41:58.328893Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"Let's make the values of the column \"Item_Fat_Content\" uniform ","metadata":{}},{"cell_type":"code","source":"data.replace({'Item_Fat_Content': {'low fat':'Low Fat','LF':'Low Fat', 'reg':'Regular'}}, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:42:09.149796Z","iopub.execute_input":"2021-10-06T17:42:09.150126Z","iopub.status.idle":"2021-10-06T17:42:09.160412Z","shell.execute_reply.started":"2021-10-06T17:42:09.150098Z","shell.execute_reply":"2021-10-06T17:42:09.159606Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"Check if we still have null values in our datasets","metadata":{}},{"cell_type":"markdown","source":"Now, let's manage categorical columns with labelencoder","metadata":{}},{"cell_type":"code","source":"for col in cat_features:\n    encoder = LabelEncoder()\n    data[col] = encoder.fit_transform(data[col])","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:43:27.166454Z","iopub.execute_input":"2021-10-06T17:43:27.166793Z","iopub.status.idle":"2021-10-06T17:43:27.193285Z","shell.execute_reply.started":"2021-10-06T17:43:27.166764Z","shell.execute_reply":"2021-10-06T17:43:27.192387Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-06T17:43:29.554493Z","iopub.execute_input":"2021-10-06T17:43:29.554984Z","iopub.status.idle":"2021-10-06T17:43:29.569166Z","shell.execute_reply.started":"2021-10-06T17:43:29.554955Z","shell.execute_reply":"2021-10-06T17:43:29.568090Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"## Model Training and parameters tuning\n\nI choose RandomForestRegressor as model. Let's prepare data. I will use gridsearchcv to find best parameters of my model.","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(data[features], data[target], test_size=0.3, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T18:18:46.961182Z","iopub.execute_input":"2021-10-06T18:18:46.961561Z","iopub.status.idle":"2021-10-06T18:18:46.972668Z","shell.execute_reply.started":"2021-10-06T18:18:46.961531Z","shell.execute_reply":"2021-10-06T18:18:46.971428Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":"Let's try a simple model on our data","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=1000, random_state=0)\nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)\nscore = r2_score(y_valid, preds)\nprint('R2:', score)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T19:58:04.437390Z","iopub.execute_input":"2021-10-06T19:58:04.437749Z","iopub.status.idle":"2021-10-06T19:58:33.977240Z","shell.execute_reply.started":"2021-10-06T19:58:04.437716Z","shell.execute_reply":"2021-10-06T19:58:33.976400Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"Now, we will try to improve model by tuning model parameters","metadata":{}},{"cell_type":"code","source":"model = RandomForestRegressor(random_state=0)\n\nmy_pipeline = Pipeline(steps=[\n                              ('rfr', model)\n                             ])\n\nparam_grid = [\n{'rfr__n_estimators': [100, 1000], 'rfr__max_features': [\"auto\", \"log2\"], \n 'rfr__max_depth': [None, 25]}\n]\n\ngrid_search_rfr = GridSearchCV(my_pipeline, param_grid, cv=4, scoring='r2', n_jobs=-1)\ngrid_search_rfr.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T20:13:58.554645Z","iopub.execute_input":"2021-10-06T20:13:58.554984Z","iopub.status.idle":"2021-10-06T20:18:28.898547Z","shell.execute_reply.started":"2021-10-06T20:13:58.554951Z","shell.execute_reply":"2021-10-06T20:18:28.897407Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"print(\"Best parameter (CV score=%0.3f):\" % grid_search_rfr.best_score_)\nprint(grid_search_rfr.best_params_)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T20:20:20.499586Z","iopub.execute_input":"2021-10-06T20:20:20.499962Z","iopub.status.idle":"2021-10-06T20:20:20.505325Z","shell.execute_reply.started":"2021-10-06T20:20:20.499929Z","shell.execute_reply":"2021-10-06T20:20:20.504312Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"model = RandomForestRegressor(n_estimators=1000, random_state=0, max_depth=None, max_features='log2')\nmodel.fit(X_train, y_train)\npreds = model.predict(X_valid)\nscore = r2_score(y_valid, preds)\nprint('R2:', score)","metadata":{"execution":{"iopub.status.busy":"2021-10-06T19:49:45.607696Z","iopub.execute_input":"2021-10-06T19:49:45.608013Z","iopub.status.idle":"2021-10-06T19:49:56.689634Z","shell.execute_reply.started":"2021-10-06T19:49:45.607985Z","shell.execute_reply":"2021-10-06T19:49:56.688685Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"This is the end of notebook, we started by making some assumptions, analyzes and then preprocessing the data and finally learning the model and tuning parameters. The score I have can be improved by using another model like XGRegressor, but also by doing some feature engineering which is also important to get a better score. In my work, I did not make any transformation on the skwed target's data. I will try to improve that point. I have seen a few articles that deal with this and I tried the logarithmic transformation which did not give good results. Reason why I did not keep it in the final version of the notebook.\n\nI will appreciate if anyone has a suggestion or question for me, please feel free to write a comment. Thank you:)","metadata":{}}]}